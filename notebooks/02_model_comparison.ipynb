{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md_title",
   "metadata": {},
   "source": [
    "# II. Model Comparison\n",
    "\n",
    "## Objective\n",
    "\n",
    "Train and compare two supervised tree-based ensemble classifiers on the fault-detection task:\n",
    "\n",
    "| Model | Role |\n",
    "|---|---|\n",
    "| Random Forest | Bagged ensemble of decision trees |\n",
    "| XGBoost | Sequential gradient-boosted trees |\n",
    "\n",
    "Tree-based ensemble methods are selected because each landing event is treated as an **independent observation** represented by a fixed-length scalar feature vector. These models do not assume temporal continuity across observations, align naturally with event-level tabular data, and provide interpretable feature-importance outputs.\n",
    "\n",
    "## Target Variable\n",
    "\n",
    "`Fault_Code` — four mutually exclusive ground-truth classes injected into the simulation:\n",
    "\n",
    "| Code | Failure Mode | Physical Effect |\n",
    "|---|---|---|\n",
    "| 0 | Normal Operation | Nominal stiffness and damping |\n",
    "| 1 | Nitrogen Gas Leak | Reduced gas-spring stiffness (K ↓) |\n",
    "| 2 | Worn Seal | Reduced hydraulic damping (B ↓) |\n",
    "| 3 | Early Structural Degradation | Combined stiffness and damping loss |\n",
    "\n",
    "Physics-derived features (ζ and ω_n) are included alongside raw signals — they are directly sensitive to the K and B changes that define faults 1 through 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_setup",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all required libraries. RANDOM_STATE=42 ensures every stochastic step (train/test split, model initialisation) is fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")  # consistent styling for all plots\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_data",
   "metadata": {},
   "source": [
    "## Data Loading and Feature Engineering\n",
    "\n",
    "### Dataset origin\n",
    "\n",
    "The CSV contains **1 500 simulated landing events**. Each row is one landing event; fault conditions were injected at the simulation level, so `Fault_Code` reflects the ground-truth state of the digital twin.\n",
    "\n",
    "### Column reference\n",
    "\n",
    "| Column | Unit | Type | Description |\n",
    "|---|---|---|---|\n",
    "| `RunID` | — | Identifier | Unique simulation run index; excluded from modelling |\n",
    "| `Max_Deflection` | m | Sensor signal | Peak shock-strut compression during impact |\n",
    "| `Max_Velocity` | m/s | Sensor signal | Maximum vertical piston velocity during impact |\n",
    "| `Settling_Time` | s | Sensor signal | Time for oscillations to decay to within 2 % of steady state — directly sensitive to damping faults |\n",
    "| `Mass` | kg | Simulation input | Dornier 228 landing mass, swept 3 000 – 6 400 kg across runs |\n",
    "| `K_Stiffness` | N/m | Latent physics param | Gas-spring stiffness of the strut; exposed by the digital twin, not a physical sensor |\n",
    "| `B_Damping` | N·s/m | Latent physics param | Hydraulic damping coefficient; latent parameter from the Simscape model |\n",
    "| `Fault_Code` | — | Target (class) | Ground-truth failure mode injected into the simulation |\n",
    "| `RUL` | % | Target (regression) | Remaining Useful Life; reserved for a regression task, not used here |\n",
    "\n",
    "### Physics-derived features\n",
    "\n",
    "Two features are computed from the latent Simscape parameters to make the physical fault signatures explicit:\n",
    "\n",
    "- **ζ (damping ratio)** = B / (2√(K · m)) — dimensionless. A **Worn Seal** reduces B → ζ drops. A **N₂ Gas Leak** reduces K → both ζ and ω_n shift simultaneously.\n",
    "- **ω_n (natural frequency)** = √(K / m) — rad/s. Falls when K decreases (faults 1 and 3).\n",
    "\n",
    "In a real operational context, the stiffness (K) and damping (B) coefficients of a landing gear strut are not directly accessible through sensors. Instead, the damping ratio (ζ) and natural frequency (ω_n) — which encode the same physical information — can be estimated from measurable dynamic responses such as strut deflection and oscillation decay. This makes them the natural bridge between knowledge generated by the digital twin and health inference on an actual aircraft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data/raw/LandingGear_Balanced_Dataset.csv\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Physics-informed feature engineering\n",
    "df[\"zeta\"]   = df[\"B_Damping\"] / (2 * np.sqrt(df[\"K_Stiffness\"] * df[\"Mass\"])) # damping ratio\n",
    "df[\"omega_n\"] = np.sqrt(df[\"K_Stiffness\"] / df[\"Mass\"]) # natural frequency\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "print(\"Fault_Code distribution:\")\n",
    "print(df[\"Fault_Code\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_split",
   "metadata": {},
   "source": [
    "## Feature Selection and Train-Test Split\n",
    "\n",
    "The feature matrix **X** contains three observable sensor signals (`Max_Deflection`, `Max_Velocity`, `Settling_Time`), one simulation input (`Mass`), two latent physics parameters exposed by the digital twin (`K_Stiffness`, `B_Damping`), and the two derived physics features (`zeta`, `omega_n`). `RunID` and `RUL` are excluded.\n",
    "\n",
    "The dataset is split **80 / 20** with **stratification** on `Fault_Code` to preserve the class distribution in both partitions. Random Forest and XGBoost are scale-invariant tree-based models — no feature normalisation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = {\n",
    "    0: \"Normal Operation\",\n",
    "    1: \"N2 Gas Leak\",\n",
    "    2: \"Worn Seal\",\n",
    "    3: \"Early Structural Degradation\",\n",
    "}\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Max_Deflection\", \"Max_Velocity\", \"Settling_Time\",\n",
    "    \"Mass\", \"K_Stiffness\", \"B_Damping\",\n",
    "    \"zeta\", \"omega_n\",\n",
    "]\n",
    "TARGET = \"Fault_Code\"\n",
    "\n",
    "X = df[FEATURE_COLS].values\n",
    "y = df[TARGET].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train : {len(X_train):,} samples\")\n",
    "print(f\"Test  : {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_helpers",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "\n",
    "`evaluate_model` prints a full per-class classification report and returns a summary dict with accuracy and **macro-averaged** precision, recall, and F1. Macro averaging computes each metric independently per class then takes the unweighted mean — giving equal weight to every fault type regardless of class size, consistent with the paper's evaluation design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, y_true, y_pred):\n",
    "    \"\"\"Return headline metrics and print a full classification report.\"\"\"\n",
    "    labels = sorted(set(y_true))\n",
    "    print(\"=\" * 55)\n",
    "    print(f\"  {name}\")\n",
    "    print(\"=\" * 55)\n",
    "    print(classification_report(\n",
    "        y_true, y_pred,\n",
    "        target_names=[CLASS_NAMES[i] for i in labels],\n",
    "    ))\n",
    "    return {\n",
    "        \"Model\"    : name,\n",
    "        \"Accuracy\" : round(accuracy_score(y_true, y_pred), 4),\n",
    "        \"Precision\": round(precision_score(y_true, y_pred, average=\"macro\"), 4),\n",
    "        \"Recall\"   : round(recall_score(y_true, y_pred, average=\"macro\"), 4),\n",
    "        \"F1\"       : round(f1_score(y_true, y_pred, average=\"macro\"), 4),\n",
    "    }\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_rf",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A Random Forest of 300 decision trees, each trained on a bootstrap sample with a random feature subset (max_features=sqrt) at every split. Ensemble averaging reduces variance compared to a single tree. The **unscaled** feature matrix is used since tree-based models are invariant to monotonic feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_features=\"sqrt\",\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "results.append(evaluate_model(\"Random Forest\", y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_xgb",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost builds trees **sequentially**, each correcting the residual errors of the ensemble so far.\n",
    "\n",
    "| Parameter | Value | Reason |\n",
    "|---|---|---|\n",
    "| n_estimators | 300 | Number of boosting rounds |\n",
    "| learning_rate | 0.1 | Shrinks each tree contribution to reduce overfitting |\n",
    "| max_depth | 6 | Per-tree complexity limit |\n",
    "| subsample | 0.8 | Row subsampling - stochastic regularisation |\n",
    "| colsample_bytree | 0.8 | Column subsampling per tree |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    use_label_encoder=False,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "results.append(evaluate_model(\"XGBoost\", y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_summary",
   "metadata": {},
   "source": [
    "## Metric Summary\n",
    "\n",
    "Collect all per-model metrics into a single DataFrame for a side-by-side comparison, then render as a grouped bar chart so relative differences are immediately visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(results).set_index(\"Model\")\n",
    "display(summary)\n",
    "\n",
    "ax = summary.plot(kind=\"bar\", figsize=(10, 5), ylim=(0.8, 1.02), width=0.6)\n",
    "ax.set_title(\"Model comparison - classification metrics (test set)\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.tick_params(axis=\"x\", rotation=0)\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cv",
   "metadata": {},
   "source": [
    "## Cross-Validation Stability Check\n",
    "\n",
    "A single train/test split can be sensitive to which samples land in which partition. **Stratified 5-fold cross-validation** repeats evaluation across five non-overlapping folds, providing a mean and standard deviation for each model's **macro-averaged F1** score. A low standard deviation confirms the test-set result is representative and not an artefact of the particular split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_cv",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in [\n",
    "    (\"Random Forest\", rf),\n",
    "    (\"XGBoost\",       xgb),\n",
    "]:\n",
    "    scores = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        cv=cv, scoring=\"f1_macro\", n_jobs=-1,\n",
    "    )[\"test_score\"]\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name:<25} macro F1 = {scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.boxplot(cv_results.values(), labels=cv_results.keys(), patch_artist=True)\n",
    "ax.set_title(\"5-fold CV — macro F1 per fold\")\n",
    "ax.set_ylabel(\"Macro F1\")\n",
    "ax.tick_params(axis=\"x\", rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_cm",
   "metadata": {},
   "source": [
    "## Confusion Matrices\n",
    "\n",
    "A confusion matrix shows the count of true vs. predicted labels for every class combination. Diagonal cells are correct predictions; off-diagonal cells are misclassifications. Comparing matrices across models reveals which fault classes each model struggles with, not just the aggregate error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_cm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [CLASS_NAMES[i] for i in sorted(set(y_test))]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, (name, y_pred) in zip(axes, [\n",
    "    (\"Random Forest\", y_pred_rf),\n",
    "    (\"XGBoost\",       y_pred_xgb),\n",
    "]):\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_pred,\n",
    "        display_labels=class_labels,\n",
    "        cmap=\"Blues\",\n",
    "        ax=ax,\n",
    "        colorbar=False,\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    ax.tick_params(axis=\"x\", rotation=20)\n",
    "\n",
    "plt.suptitle(\"Confusion matrices — test set\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_fi",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Tree-based models expose a built-in feature-importance score - the mean decrease in impurity (Gini) across all splits that used each feature. Comparing rankings between Random Forest and XGBoost highlights the contribution of the physics-derived features (zeta, omega_n) relative to the raw sensor measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code_fi",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_rf  = pd.Series(rf.feature_importances_,  index=FEATURE_COLS).sort_values()\n",
    "fi_xgb = pd.Series(xgb.feature_importances_, index=FEATURE_COLS).sort_values()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fi_rf.plot( kind=\"barh\", ax=axes[0], color=\"#4c72b0\",\n",
    "           title=\"Random Forest - feature importance\")\n",
    "fi_xgb.plot(kind=\"barh\", ax=axes[1], color=\"#dd8452\",\n",
    "           title=\"XGBoost - feature importance\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Importance (mean decrease in impurity)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hck3t4205p",
   "metadata": {},
   "source": [
    "## Cross-Dataset Generalizability — AEROTWIN Do228 V16.1\n",
    "\n",
    "To assess whether the trained classifiers generalise beyond the primary dataset, a **cross-dataset evaluation** is performed using the **AEROTWIN Do228 V16.1** dataset (Bello Sani, 2026) as a fully independent external test set. AEROTWIN contains 9,500 simulated landing events across 11 fault classes generated by the same Dornier 228 digital twin framework.\n",
    "\n",
    "### Feature alignment\n",
    "\n",
    "Two primary dataset features are unavailable in AEROTWIN:\n",
    "\n",
    "| Feature | Status in AEROTWIN |\n",
    "|---|---|\n",
    "| `Settling_Time` | Absent — no equivalent column |\n",
    "| `Max_Deflection` | Zero variance — single value across all 9,500 records |\n",
    "\n",
    "The cross-dataset classifier therefore uses a **reduced 6-feature set**: `Max_Velocity`, `Mass`, `K_Stiffness`, `B_Damping`, `zeta`, `omega_n`. Both `zeta` and `omega_n` are computable from the structural parameters available in AEROTWIN.\n",
    "\n",
    "### Evaluation scope\n",
    "\n",
    "Only the 4 fault classes shared with the primary dataset (codes 0–3) are evaluated. AEROTWIN serves as the external test set in its entirety — no further splitting is performed. A separate pair of classifiers is retrained on the primary training set using only the 6 shared features before being applied to AEROTWIN.\n",
    "\n",
    "| Code | Fault Mode | AEROTWIN Count |\n",
    "|---|---|---|\n",
    "| 0 | Normal Operation | 1,500 |\n",
    "| 1 | N2 Gas Leak | 1,500 |\n",
    "| 2 | Worn Seal | 1,500 |\n",
    "| 3 | Early Structural Degradation | 1,000 |\n",
    "\n",
    "Due to this class imbalance, **per-class F1-scores** are reported alongside the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9vr67xa6xj",
   "metadata": {},
   "outputs": [],
   "source": [
    "AEROTWIN_PATH = Path(\"../data/raw/AEROTWIN_Dataset_Preprocessed_ZSK.csv\")\n",
    "df_aero = pd.read_csv(AEROTWIN_PATH)\n",
    "\n",
    "# Rename target column to align with primary dataset convention\n",
    "df_aero = df_aero.rename(columns={\"Class\": \"Fault_Code\"})\n",
    "\n",
    "# Filter to the 4 shared fault classes only\n",
    "SHARED_CLASSES = [0, 1, 2, 3]\n",
    "df_aero = df_aero[df_aero[\"Fault_Code\"].isin(SHARED_CLASSES)].copy()\n",
    "\n",
    "# Compute physics-derived features from the structural parameters\n",
    "df_aero[\"zeta\"]    = df_aero[\"B_Damping\"] / (2 * np.sqrt(df_aero[\"K_Stiffness\"] * df_aero[\"Mass\"]))\n",
    "df_aero[\"omega_n\"] = np.sqrt(df_aero[\"K_Stiffness\"] / df_aero[\"Mass\"])\n",
    "\n",
    "print(f\"AEROTWIN (shared classes only): {df_aero.shape[0]:,} rows\")\n",
    "print(\"Class distribution:\")\n",
    "print(df_aero[\"Fault_Code\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5g1piavkjcu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced 6-feature set — features available in both datasets\n",
    "FEATURE_COLS_6 = [\"Max_Velocity\", \"Mass\", \"K_Stiffness\", \"B_Damping\", \"zeta\", \"omega_n\"]\n",
    "\n",
    "# Extract the corresponding column indices from the original 8-feature training array\n",
    "idx6 = [FEATURE_COLS.index(f) for f in FEATURE_COLS_6]\n",
    "X_train_6 = X_train[:, idx6]\n",
    "\n",
    "# Retrain classifiers on the primary training set using only the 6 shared features\n",
    "rf6 = RandomForestClassifier(\n",
    "    n_estimators=300, max_features=\"sqrt\", n_jobs=-1, random_state=RANDOM_STATE\n",
    ")\n",
    "rf6.fit(X_train_6, y_train)\n",
    "\n",
    "xgb6 = XGBClassifier(\n",
    "    n_estimators=300, learning_rate=0.1, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8,\n",
    "    eval_metric=\"mlogloss\", use_label_encoder=False,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1,\n",
    ")\n",
    "xgb6.fit(X_train_6, y_train)\n",
    "\n",
    "print(\"6-feature classifiers retrained on primary training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hieyqg4bfgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aero = df_aero[FEATURE_COLS_6].values\n",
    "y_aero = df_aero[\"Fault_Code\"].values\n",
    "\n",
    "y_pred_rf6  = rf6.predict(X_aero)\n",
    "y_pred_xgb6 = xgb6.predict(X_aero)\n",
    "\n",
    "aero_labels = [CLASS_NAMES[i] for i in sorted(SHARED_CLASSES)]\n",
    "\n",
    "# Per-class classification reports\n",
    "for name, y_pred in [\n",
    "    (\"Random Forest — AEROTWIN (6-feat)\", y_pred_rf6),\n",
    "    (\"XGBoost — AEROTWIN (6-feat)\",       y_pred_xgb6),\n",
    "]:\n",
    "    print(\"=\" * 55)\n",
    "    print(f\"  {name}\")\n",
    "    print(\"=\" * 55)\n",
    "    print(classification_report(y_aero, y_pred, target_names=aero_labels))\n",
    "\n",
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "for ax, (name, y_pred) in zip(axes, [\n",
    "    (\"Random Forest (6-feat)\", y_pred_rf6),\n",
    "    (\"XGBoost (6-feat)\",       y_pred_xgb6),\n",
    "]):\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_aero, y_pred,\n",
    "        display_labels=aero_labels,\n",
    "        cmap=\"Oranges\",\n",
    "        ax=ax,\n",
    "        colorbar=False,\n",
    "    )\n",
    "    ax.set_title(name)\n",
    "    ax.tick_params(axis=\"x\", rotation=20)\n",
    "\n",
    "plt.suptitle(\"Cross-dataset confusion matrices — AEROTWIN (external test set)\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_findings",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Within-dataset results (primary dataset, 8-feature set, 20 % hold-out test set)\n",
    "\n",
    "| Model | Accuracy | Macro F1 | Macro Precision | Macro Recall |\n",
    "|---|---|---|---|---|\n",
    "| **Random Forest** | **0.9933** | **0.9933** | **0.9951** | **0.9917** |\n",
    "| XGBoost | 0.9900 | 0.9899 | 0.9926 | 0.9875 |\n",
    "\n",
    "**Random Forest is the best-performing model** on the within-dataset task — achieving 99.3 % accuracy and a macro F1 of 0.9933, marginally ahead of XGBoost (0.9899). Both models correctly classify nearly all 300 test-set events; the only misclassifications involve the Normal Operation class (the smallest class at 60 test samples).\n",
    "\n",
    "### Cross-validation stability (5-fold stratified, macro F1 on training set)\n",
    "\n",
    "| Model | Mean macro F1 | Std |\n",
    "|---|---|---|\n",
    "| Random Forest | 0.9902 | ±0.0049 |\n",
    "| XGBoost | 0.9981 | ±0.0024 |\n",
    "\n",
    "XGBoost achieves a higher and more stable CV score across folds. The small gap between XGBoost's CV mean (0.9981) and its test-set F1 (0.9899) indicates mild overfitting to the training distribution. Random Forest shows tighter alignment between its CV mean (0.9902) and test F1 (0.9933), suggesting more consistent generalisation.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Both models agree on the top two features: **`B_Damping`** and **`K_Stiffness`** dominate classification, accounting for ~57 % of RF importance and ~73 % of XGBoost importance. This is physically consistent — the four fault classes are defined by injecting changes to exactly these two parameters (reduced K for N2 Gas Leak, reduced B for Worn Seal). `Settling_Time` and `Max_Velocity` provide secondary discriminative signal. `Max_Deflection` carries near-zero importance in both models (RF: 0.0005, XGBoost: 0.0000), confirming it adds no diagnostic value. The physics-derived feature `zeta` contributes meaningfully in RF (0.0756) but is redundant in XGBoost, which captures the same information directly through `B_Damping` and `K_Stiffness`.\n",
    "\n",
    "### Cross-dataset generalisation (AEROTWIN, 6-feature set, external test set)\n",
    "\n",
    "| Model | Accuracy | Macro F1 |\n",
    "|---|---|---|\n",
    "| Random Forest (6-feat) | 0.39 | 0.2522 |\n",
    "| XGBoost (6-feat) | 0.48 | 0.3933 |\n",
    "\n",
    "Both models transfer poorly to AEROTWIN. Random Forest fails to predict N2 Gas Leak and Early Structural Degradation entirely (precision = recall = 0). XGBoost predicts N2 Gas Leak with high precision (0.99) but very low recall (0.33), and cannot identify Early Structural Degradation at all. The substantial **domain shift** between datasets — AEROTWIN covers a much wider operational envelope across all four shared features — explains the degradation. Models trained on the narrow primary parameter space cannot reliably discriminate fault signatures under the broader AEROTWIN conditions. This finding underscores the need for domain adaptation or retraining when transferring models across different simulation configurations.\n",
    "\n",
    "The best-performing model (Random Forest, macro F1 = 0.9933 on the primary dataset) is carried forward to `03_model_evaluation.ipynb` for deeper error analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
